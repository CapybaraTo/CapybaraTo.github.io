<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>爬虫+数据分析【day1】 | Capybarato</title><meta name="author" content="Capybara"><meta name="copyright" content="Capybara"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="学习方法不要边听边记，不要花大把的时间记笔记。照抄笔记没有意义。 记忆的不深刻：用自己的话术记笔记，自己的话术要白话一点。只有自己有了深刻的理解，才能把它用白话讲出来，不理解才会背专业的话术。 课上一点笔记都不要做，课下看一遍老师的笔记，并按自己的理解与话术白话一点地做一个总结即可。不要过于担心自己的思路偏差，只要自己总结的概念在使用过程中没有任何问题就是对的思路。 day11 开发环境介绍 an">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫+数据分析【day1】">
<meta property="og:url" content="http://example.com/2023/10/30/%E7%88%AC%E8%99%AB-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%90day1%E3%80%91/index.html">
<meta property="og:site_name" content="Capybarato">
<meta property="og:description" content="学习方法不要边听边记，不要花大把的时间记笔记。照抄笔记没有意义。 记忆的不深刻：用自己的话术记笔记，自己的话术要白话一点。只有自己有了深刻的理解，才能把它用白话讲出来，不理解才会背专业的话术。 课上一点笔记都不要做，课下看一遍老师的笔记，并按自己的理解与话术白话一点地做一个总结即可。不要过于担心自己的思路偏差，只要自己总结的概念在使用过程中没有任何问题就是对的思路。 day11 开发环境介绍 an">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/pa.png">
<meta property="article:published_time" content="2023-10-30T14:07:29.000Z">
<meta property="article:modified_time" content="2023-10-30T14:14:20.457Z">
<meta property="article:author" content="Capybara">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/pa.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/10/30/%E7%88%AC%E8%99%AB-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%90day1%E3%80%91/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '爬虫+数据分析【day1】',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-10-30 22:14:20'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/loading.gif" data-original="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/pa.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Capybarato"><span class="site-name">Capybarato</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">爬虫+数据分析【day1】</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-30T14:07:29.000Z" title="发表于 2023-10-30 22:07:29">2023-10-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-10-30T14:14:20.457Z" title="更新于 2023-10-30 22:14:20">2023-10-30</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="爬虫+数据分析【day1】"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="学习方法"><a href="#学习方法" class="headerlink" title="学习方法"></a>学习方法</h1><p>不要边听边记，不要花大把的时间记笔记。照抄笔记没有意义。</p>
<p>记忆的不深刻：用自己的话术记笔记，自己的话术要白话一点。只有自己有了深刻的理解，才能把它用白话讲出来，不理解才会背专业的话术。</p>
<p>课上一点笔记都不要做，课下看一遍老师的笔记，并按自己的理解与话术白话一点地做一个总结即可。不要过于担心自己的思路偏差，只要自己总结的概念在使用过程中没有任何问题就是对的思路。</p>
<h1 id="day1"><a href="#day1" class="headerlink" title="day1"></a>day1</h1><h1 id="1-开发环境介绍"><a href="#1-开发环境介绍" class="headerlink" title="1 开发环境介绍"></a>1 开发环境介绍</h1><ul>
<li><p><strong>anaconda</strong>：基于数据分析和机器学习的<strong>集成环境</strong></p>
</li>
<li><p><strong>jupyter</strong>：anaconda 提供的一个基于浏览器的可视化开发的工具</p>
</li>
</ul>
<h1 id="2-jupyter的基本使用"><a href="#2-jupyter的基本使用" class="headerlink" title="2 jupyter的基本使用"></a>2 jupyter的基本使用</h1><ul>
<li><p>在终端录入jupyter notebook的指令启动jupyter可视化的开发工具</p>
</li>
<li><p>jupyter notebook的指令录入对应的默认的目录结构就是终端对应的目录结构</p>
</li>
<li><p>new-&gt;text file：新建一个任意后缀的文本文件，可以写代码但不能执行，在终端里利用python工作可以执行。</p>
</li>
<li><p>new-&gt;python 3 ：新建一个基于jupyter的源文件（xxx.ipynb）</p>
</li>
<li><p>ceil：jupyter源文件中的一个编辑行。</p>
</li>
<li><p>ceil是可以分为两种不同的模式：</p>
<ul>
<li>code模式：用来编写和执行代码</li>
<li>mardown模式：编写笔记。里面写代码不能执行</li>
</ul>
</li>
<li><p>快捷键的使用</p>
<ul>
<li><p>插入ceil：按a、b</p>
</li>
<li><p>删除ceil：x</p>
</li>
<li><p>撤销上一次：z</p>
</li>
<li><p>执行ceil：shift+enter  不论是code模式还是markdown模式都需要执行才能看到最后的结果。</p>
</li>
<li><p>切换ceil的模式：</p>
<ul>
<li>y：将markdown模式的ceil切换成code模式</li>
<li>m：将code切换为markdown</li>
</ul>
</li>
<li><p>打开帮助文档</p>
<ul>
<li>shift+tab：快速知道方法的含义、参数、例子等</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="3-爬虫"><a href="#3-爬虫" class="headerlink" title="3 爬虫"></a>3 爬虫</h1><p>概念：就是通过编写程序，让其模拟浏览器上网，然后去互联网上抓取数据的过程</p>
<ul>
<li>模拟：浏览器就是一款天然的爬虫工具!</li>
<li>抓取:：抓取一整张数据，抓取一整张数据中的局部数据</li>
</ul>
<p>爬虫的分类:</p>
<ul>
<li>通用爬虫（数据的爬取）：抓取一整张页面源码数据</li>
<li>聚焦爬虫（数据解析）：抓取局部的指定的数据。基于通用爬虫</li>
<li>增量式爬虫（数据的更新）：监测网站数据更新的情况!抓取网站最新更新出来的数据。</li>
<li>分布式：</li>
</ul>
<p>反爬机制</p>
<ul>
<li>一些网站的后台会设定相关的机制阻止爬虫程序进行数据的爬取。</li>
</ul>
<p>反反爬策略</p>
<ul>
<li>爬虫需要制定相关的策略破解反爬机制，从而可以爬取到网站的数据。</li>
</ul>
<p><strong>第一个反爬机制</strong></p>
<ul>
<li>robots协议: 存在于网站服务器的一个文本协议。指明了该网站中哪些数据可以爬取哪些不可以爬取。（防君子不防小人）</li>
</ul>
<img src="/img/loading.gif" data-original="/images/image-20231016165552576.png" alt="image-20231016165552576" style="zoom:50%;" />

<p>爬虫风险：</p>
<ul>
<li>爬虫干扰了被访问网站的正常运营;</li>
<li>爬虫抓取了受到法律保护的特定类型的数据或信息</li>
</ul>
<h2 id="3-1-requests的基本操作"><a href="#3-1-requests的基本操作" class="headerlink" title="3.1 requests的基本操作"></a>3.1 requests的基本操作</h2><p>urllib模块：基于模拟浏览器上网的模块，网络请求模块。（基本没人用了）</p>
<p>requests模块：基于网络请求的模块。作用：模拟浏览器上网。</p>
<p>requests模块的编码流程：</p>
<ol>
<li>指定url</li>
<li>发起请求</li>
<li>获取响应数据（爬取的数据）</li>
<li>对数据持久化存储</li>
</ol>
<h3 id="爬取搜狗首页的页面源码数据"><a href="#爬取搜狗首页的页面源码数据" class="headerlink" title="#爬取搜狗首页的页面源码数据"></a><strong>#爬取搜狗首页的页面源码数据</strong></h3><pre><code>#1.爬取搜狗首页的页面源码数据
url = &#39;https://www.sogou.com/&#39;
response = requests.get(url=url)
page_text = response.text #text返回的是字符串形式的响应数据
with open(&#39;./sogou.html&#39;,&#39;w&#39;,encoding=&#39;utf-8&#39;) as fp:
    fp.write(page_text)   //写到文件里
#分别对应如上四步
</code></pre>
<p><code>with open() as fp:</code>是 Python 中打开文件的一种常用方式。它的作用是打开一个文件，并将其赋值给一个变量 fp。</p>
<p> with关键字 可以在语句结束后，关闭文件流。不用with关键字，文件会被python垃圾回收关闭。</p>
<p>open(‘file’, mode)第一个参数是包含文件名的字符串。第二个参数是另一个字符串，其中包含一些描述文件使用方式的字符，如 ‘r’，表示文件只能读取，‘w’ 表示只能写入。<code>encoding=&#39;utf-8&#39;</code>以utf-8的格式存入</p>
<h3 id="简易的网页采集器"><a href="#简易的网页采集器" class="headerlink" title="#简易的网页采集器"></a><strong>#简易的网页采集器</strong></h3><pre><code>#2.简易的网页采集器
#涉及到的知识点：参数动态化，UA伪装，乱码的处理
word = input(&#39;enter a key word:&#39;)  //输入我想搜素的
url = &#39;https://www.sogou.com/web&#39;
#参数动态化：将请求参数封装成字典作用到get方法的params参数中
params = &#123;
    &#39;query&#39;:word
&#125;
response = requests.get(url=url,params=params)
page_text = response.text
fileName = word+&#39;.html&#39;   //起一个名字，存爬取数据的文件
with open(fileName,&#39;w&#39;,encoding=&#39;utf-8&#39;) as fp:
    fp.write(page_text)
print(word,&#39;下载成功！！！&#39;)
</code></pre>
<p><code>url = &#39;https://www.sogou.com/web?query=jay&#39;</code> 只能请求jay的，想我输入什么就请求什么–&gt;参数动态化</p>
<ul>
<li>上述代码出现的问题：<ul>
<li><strong>乱码问题</strong></li>
<li><strong>爬取数据丢失</strong></li>
</ul>
</li>
</ul>
<pre><code>#乱码处理
word = input(&#39;enter a key word:&#39;)
url = &#39;https://www.sogou.com/web&#39;
#参数动态化：将请求参数封装成字典作用到get方法的params参数中
params = &#123;
    &#39;query&#39;:word
&#125;
response = requests.get(url=url,params=params)
#可以修改响应数据的编码
response.encoding = &#39;utf-8&#39;#手动修改了响应对象的编码格式
page_text = response.text
fileName = word+&#39;.html&#39;
with open(fileName,&#39;w&#39;,encoding=&#39;utf-8&#39;) as fp:
    fp.write(page_text)
print(word,&#39;下载成功！！！&#39;)
</code></pre>
<p>显示 检测到异常的访问请求。</p>
<ul>
<li><p>什么叫做异常的访问请求？</p>
<ul>
<li>在爬虫中正常的访问请求指的是通过真实的浏览器发起的访问请求。</li>
<li>异常的访问请求：通过非浏览器发起的请求。（爬虫程序模拟的请求发送）</li>
</ul>
</li>
<li><p>正常的访问请求和异常的访问请求的判别方式是什么？</p>
<ul>
<li>是通过请求头中的User-Agent判别。</li>
<li>User-Agent：请求载体的身份标识</li>
<li>目前请求的载体可以是：浏览器，爬虫</li>
</ul>
</li>
<li><p>反爬机制：</p>
<ul>
<li>UA检测：网站后台会检测请求载体的身份标识（UA）是不是浏览器。<ul>
<li>是：正常的访问请求</li>
<li>不是：异常的访问请求</li>
</ul>
</li>
</ul>
</li>
<li><p>反反爬策略：</p>
<ul>
<li><h3 id="UA伪装："><a href="#UA伪装：" class="headerlink" title="UA伪装："></a><strong>UA伪装</strong>：</h3><ul>
<li>大部分网站都需要进行下UA伪装</li>
<li>将爬虫对应的请求载体身份标识伪装&#x2F;篡改成浏览器的身份标识</li>
<li>请求的header里的User-Agent: Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.122 Safari&#x2F;537.36    不同的浏览器是不同的这个字符串</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>#UA伪装
word = input(&#39;enter a key word:&#39;)
url = &#39;https://www.sogou.com/web&#39;
#参数动态化：将请求参数封装成字典作用到get方法的params参数中
params = &#123;
    &#39;query&#39;:word
&#125;
#UA伪装
headers = &#123;
    &quot;User-Agent&quot;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39;
&#125;
#将伪装的UA作用到了请求的请求头中
response = requests.get(url=url,params=params,headers=headers)
#可以修改响应数据的编码
response.encoding = &#39;utf-8&#39;#手动修改了响应对象的编码格式
page_text = response.text
fileName = word+&#39;.html&#39;
with open(fileName,&#39;w&#39;,encoding=&#39;utf-8&#39;) as fp:
    fp.write(page_text)
print(word,&#39;下载成功！！！&#39;)
</code></pre>
<p><strong>&#x3D;&#x3D;总结&#x3D;&#x3D;：</strong></p>
<ul>
<li>get方法的参数：<ul>
<li>url</li>
<li>params</li>
<li>headers</li>
</ul>
</li>
<li>get方法的返回值：<ul>
<li>response</li>
</ul>
</li>
<li>response的属性：<ul>
<li>text：字符串形式的响应数据</li>
</ul>
</li>
</ul>
<h2 id="3-2-动态加载的数据"><a href="#3-2-动态加载的数据" class="headerlink" title="3.2 动态加载的数据"></a>3.2 动态加载的数据</h2><h3 id="爬取豆瓣网中的电影详情数据"><a href="#爬取豆瓣网中的电影详情数据" class="headerlink" title="#爬取豆瓣网中的电影详情数据"></a><strong>#爬取豆瓣网中的电影详情数据</strong></h3><ul>
<li>url：<a target="_blank" rel="noopener" href="https://movie.douban.com/typerank?type_name=%E7%A7%91%E5%B9%BB&type=17&interval_id=100:90&action=">https://movie.douban.com/typerank?type_name&#x3D;%E7%A7%91%E5%B9%BB&amp;type&#x3D;17&amp;interval_id&#x3D;100:90&amp;action&#x3D;</a></li>
<li>涉及到的重点：动态加载数据</li>
</ul>
<p>滚轮向下不断地加载有新的数据。网页地址没变点但页面刷新，做的是一个局部刷新，发动的是ajax的请求。</p>
<p><strong>分析网站</strong>：</p>
<ul>
<li>当滚轮滑动到底部的时候，页面会发起ajax请求，且请求到一组电影详情数据。</li>
<li>当滚轮不滑动的时候，页面显示的电影数据通过对浏览器地址栏的url发起请求是请求不到的。（当要爬数据时，先从开发者工具里看看当前url请求下的response有没有要爬取的信息）</li>
</ul>
<p><strong>动态加载的数据</strong></p>
<ul>
<li>可见非即可得</li>
<li><strong>当我们对一个陌生的网站进行指定数据爬取之前，我们在写代码之前必须要做的一个事情就是校验你想要爬取的数据是否为动态加载的数据</strong>。</li>
<li>概念：通过非浏览器地址栏url请求到的数据（另外的一个新的请求请求到的数据），即地址栏的url实际上请求不到的<ul>
<li>是动态加载的数据<ul>
<li>基于抓包工具进行全局搜索，锁定动态加载数据对应的数据包即可。从数据包中提取请求的url和请求方式和请求参数。</li>
<li>怎么做：在任意一个network数据包上ctrl+F 搜索动态加载的数据定位</li>
</ul>
</li>
<li>不是动态加载的数据<ul>
<li>直接对地址栏的url发起请求就可以获取指定数据</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>response.text返回的是字符串数据，序列化，用response.json()，就不会返回一个字符串对象，而是一个列表</p>
<p><img src="/img/loading.gif" data-original="/images/image-20231016193441251.png" alt="image-20231016193441251"></p>
<p>该数据包的url是<code>&#39;https://movie.douban.com/j/chart/top_list&#39;</code>，该数据包是<strong>get请求</strong>。</p>
<pre><code>url = &#39;https://movie.douban.com/j/chart/top_list&#39;
#参数动态化
params = &#123;
    &quot;type&quot;: &quot;17&quot;,   
    &quot;interval_id&quot;: &quot;100:90&quot;,
    &quot;action&quot;: &quot;&quot;,
    &quot;start&quot;: &quot;20&quot;,
    &quot;limit&quot;: &quot;10&quot;,
&#125;
response = requests.get(url=url,headers=headers,params=params)
page_text = response.json() #json返回的是序列号好的对象
#将电影名称和评分进行解析
for dic in page_text:
    name = dic[&#39;title&#39;]
    score = dic[&#39;score&#39;]
    print(name+&#39;:&#39;+score)
</code></pre>
<img src="/img/loading.gif" data-original="/images/image-20231016193908951.png" alt="image-20231016193908951" style="zoom: 50%;" />

<p>**&#x3D;&#x3D;总结&#x3D;&#x3D;**：</p>
<ul>
<li>问题：如何检测页面中的数据是否为动态加载的数据？<ul>
<li>基于抓包工具进行局部搜索<ul>
<li>搜索到：不是动态加载数据</li>
<li>搜索不到：是动态加载数据</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="肯德基餐厅查询"><a href="#肯德基餐厅查询" class="headerlink" title="#肯德基餐厅查询"></a>#<strong>肯德基餐厅查询</strong></h3><ul>
<li><p>肯德基餐厅查询：<a target="_blank" rel="noopener" href="http://www.kfc.com.cn/kfccda/storelist/index.aspx">http://www.kfc.com.cn/kfccda/storelist/index.aspx</a></p>
</li>
<li><p>分析：</p>
<ul>
<li><p>通过测试发现，数据为动态加载数据</p>
</li>
<li><p>通过抓包工具的全局搜索捕获动态加载数据</p>
<p>全局搜索后，url是’<a target="_blank" rel="noopener" href="http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword%27%EF%BC%8C%E8%AF%B7%E6%B1%82%E6%98%AFpost%E8%AF%B7%E6%B1%82%EF%BC%8C%E6%90%BA%E5%B8%A6%E4%BA%86%E5%8F%82%E6%95%B0%E3%80%82data%E5%B0%B1%E6%98%AF%E8%AF%B7%E6%B1%82%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%8C%E5%B0%81%E8%A3%85%E4%B8%BA%E9%94%AE%E5%80%BC%E5%AF%B9%E7%9A%84%E5%BD%A2%E5%BC%8F%E3%80%82">http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&#39;，请求是post请求，携带了参数。data就是请求的参数，封装为键值对的形式。</a></p>
<p>基本所有的requests都要带上headers</p>
</li>
</ul>
</li>
</ul>
<pre><code>url = &#39;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&#39;
data = &#123;
    &quot;cname&quot;: &quot;&quot;,
    &quot;pid&quot;: &quot;&quot;,
    &quot;keyword&quot;: &quot;北京&quot;,
    &quot;pageIndex&quot;: &quot;1&quot;,
    &quot;pageSize&quot;: &quot;10&quot;,
&#125;
#参数：data是用来实现参数动态化，等同于get方法中的params参数的作用
response = requests.post(url=url,headers=headers,data=data)
page_text = response.json()
for dic in page_text[&#39;Table1&#39;]:
    pos = dic[&#39;addressDetail&#39;]
    print(pos)
</code></pre>
<p>目前只有当前页面的10条，想获取7页所有的数据</p>
<pre><code>#想要获取所有页码对应的位置信息
url = &#39;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&#39;
for pageNum in range(1,8):
    data = &#123;
        &quot;cname&quot;: &quot;&quot;,
        &quot;pid&quot;: &quot;&quot;,
        &quot;keyword&quot;: &quot;北京&quot;,
        &quot;pageIndex&quot;: str(pageNum),
        &quot;pageSize&quot;: &quot;10&quot;,
    &#125;
    #参数：data是用来实现参数动态化，等同于get方法中的params参数的作用
    response = requests.post(url=url,headers=headers,data=data)
    page_text = response.json()
    for dic in page_text[&#39;Table1&#39;]:
        pos = dic[&#39;addressDetail&#39;]
        print(pos)
</code></pre>
<h3 id="爬取药监总局中的企业详情数据"><a href="#爬取药监总局中的企业详情数据" class="headerlink" title="#爬取药监总局中的企业详情数据"></a>#<strong>爬取药监总局中的企业详情数据</strong></h3><ul>
<li><p>需求：爬取药监总局中的企业详情数据，每一家企业详情页对应的详情数据（爬取前5页企业）</p>
</li>
<li><p>url：<a target="_blank" rel="noopener" href="http://125.35.6.84:81/xk/">http://125.35.6.84:81/xk/</a></p>
</li>
<li><p>分析：</p>
<ul>
<li><strong>先看企业详情页数据</strong>是否为动态加载数据？<ul>
<li>基于抓包工具进行局部搜索。发现为动态加载数据</li>
</ul>
</li>
<li>捕获动态加载的数据<ul>
<li>基于抓包工具进行全局搜索。</li>
<li>定位到的数据包提取的<ul>
<li>url：<ul>
<li><a target="_blank" rel="noopener" href="http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById">http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById</a></li>
<li><a target="_blank" rel="noopener" href="http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById">http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById</a></li>
</ul>
</li>
<li>请求参数：<ul>
<li>id: 536878abac734332ae06dcb1a3fbd14a</li>
<li>id: 950d66fbf8714fbc9e799010e483d2d5</li>
</ul>
</li>
</ul>
</li>
<li>结论：每一家企业详情数据对应的请求url和请求方式都是一样的，只有请求参数id的值不一样。</li>
<li><strong>如果我们可以将每一家企业的id值捕获，则就可以将每一家企业详情数据进行爬取。</strong></li>
</ul>
</li>
<li>捕获企业的id<ul>
<li>企业的id表示的就是唯一的一家企业。我们就猜测企业id可能会和企业名称捆绑在一起。</li>
<li>在首页中会有不同的企业名称，则我们就基于抓包工具对首页的数据包进行全局搜索（企业名称）<ul>
<li>url：<a target="_blank" rel="noopener" href="http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList">http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList</a></li>
<li>方式：post</li>
<li>请求参数：<ul>
<li>on&#x3D;true&amp;page&#x3D;1&amp;pageSize&#x3D;15&amp;productName&#x3D;&amp;conditionType&#x3D;1&amp;applyname&#x3D;&amp;applysn&#x3D;</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>#获取每一家企业的id值，去首页分析查找对应企业的id值
url = &#39;http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList&#39;
data = &#123;
    &#39;on&#39;: &#39;true&#39;,
    &#39;page&#39;: &#39;1&#39;,
    &#39;pageSize&#39;: &#39;15&#39;,
    &#39;productName&#39;: &#39;&#39;,
    &#39;conditionType&#39;: &#39;1&#39;,
    &#39;applyname&#39;: &#39;&#39;,
    &#39;applysn&#39;: &#39;&#39;,
&#125;
response = requests.post(url=url,headers=headers,data=data)
all_company_list = response.json()[&#39;list&#39;]
for dic in all_company_list:
    _id = dic[&#39;ID&#39;]
#     print(_id)
    #将id作为请求企业详情数据url的请求参数  在for循环内
    detail_url = &#39;http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById&#39;
    data = &#123;
        &#39;id&#39;:_id
    &#125;
    response = requests.post(url=detail_url,headers=headers,data=data)
    company_detail_dic = response.json()
    person_name = company_detail_dic[&#39;businessPerson&#39;]
    addr = company_detail_dic[&#39;epsProductAddress&#39;]
    print(person_name,addr)
</code></pre>
<pre><code>#捕获多页数据
#获取每一家企业的id值，去首页分析查找对应企业的id值
url = &#39;http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList&#39;
for page in range(1,6):
    data = &#123;
        &#39;on&#39;: &#39;true&#39;,
        &#39;page&#39;: str(page),
        &#39;pageSize&#39;: &#39;15&#39;,
        &#39;productName&#39;: &#39;&#39;,
        &#39;conditionType&#39;: &#39;1&#39;,
        &#39;applyname&#39;: &#39;&#39;,
        &#39;applysn&#39;: &#39;&#39;,
    &#125;
    response = requests.post(url=url,headers=headers,data=data)
    all_company_list = response.json()[&#39;list&#39;]
    for dic in all_company_list:
        _id = dic[&#39;ID&#39;]
    #     print(_id)
        #将id作为请求企业详情数据url的请求参数
        detail_url = &#39;http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById&#39;
        data = &#123;
            &#39;id&#39;:_id
        &#125;
        response = requests.post(url=detail_url,headers=headers,data=data)
        company_detail_dic = response.json()
        person_name = company_detail_dic[&#39;businessPerson&#39;]
        addr = company_detail_dic[&#39;epsProductAddress&#39;]
        print(person_name,addr)
</code></pre>
<p>data和params的区别，**&#x3D;&#x3D;data对应post方法，params对应get方法&#x3D;&#x3D;**，区别仅此而已，没有其他区别</p>
<h2 id="3-3-爬取图片数据"><a href="#3-3-爬取图片数据" class="headerlink" title="3.3 爬取图片数据"></a>3.3 爬取图片数据</h2><h3 id="站长素材图片数据爬取"><a href="#站长素材图片数据爬取" class="headerlink" title="#站长素材图片数据爬取"></a>#站长素材图片数据爬取</h3><p>两种方式：</p>
<ul>
<li>方式1：requests</li>
<li>方式2：urllib</li>
</ul>
<pre><code>#requests
url = &#39;http://pics.sc.chinaz.com/files/pic/pic9/201908/zzpic19447.jpg&#39;
response = requests.get(url=url,headers=headers)
img_data = response.content #content返回的是bytes类型的响应数据
with open(&#39;./123.png&#39;,&#39;wb&#39;) as fp:
    fp.write(img_data)
</code></pre>
<pre><code>#urllib
from urllib import request
url = &#39;http://pics.sc.chinaz.com/files/pic/pic9/201908/zzpic19447.jpg&#39;
request.urlretrieve(url=url,filename=&#39;./456.png&#39;)
</code></pre>
<p>注意#urllib 就是<code>request</code>没有s</p>
<p><code>request.urlretrieve</code></p>
<ul>
<li>问题：两种图片爬取的方式的主要区别有哪些？<ul>
<li>requests的方式可以实现UA伪装，而urlib无法实现UA伪装</li>
</ul>
</li>
</ul>
<p>如何批量获取图片url地址</p>
<p>用正则进行数据解析</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Capybara</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/10/30/%E7%88%AC%E8%99%AB-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%90day1%E3%80%91/">http://example.com/2023/10/30/爬虫-数据分析【day1】/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Capybarato</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/pa.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/10/18/%E7%94%A8vscode%E5%88%B7leetcode/" title="用vscode刷leetcode"><img class="cover" src="/img/loading.gif" data-original="/img/vscode.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">用vscode刷leetcode</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Capybara</div><div class="author-info__description">想去银河系看热闹</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">4</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/CapybaraTo" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1023536640@qq.com@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">学习方法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#day1"><span class="toc-number">2.</span> <span class="toc-text">day1</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.</span> <span class="toc-text">1 开发环境介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-jupyter%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">4.</span> <span class="toc-text">2 jupyter的基本使用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E7%88%AC%E8%99%AB"><span class="toc-number">5.</span> <span class="toc-text">3 爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-requests%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">5.1.</span> <span class="toc-text">3.1 requests的基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E6%90%9C%E7%8B%97%E9%A6%96%E9%A1%B5%E7%9A%84%E9%A1%B5%E9%9D%A2%E6%BA%90%E7%A0%81%E6%95%B0%E6%8D%AE"><span class="toc-number">5.1.1.</span> <span class="toc-text">#爬取搜狗首页的页面源码数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%98%93%E7%9A%84%E7%BD%91%E9%A1%B5%E9%87%87%E9%9B%86%E5%99%A8"><span class="toc-number">5.1.2.</span> <span class="toc-text">#简易的网页采集器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UA%E4%BC%AA%E8%A3%85%EF%BC%9A"><span class="toc-number">5.1.3.</span> <span class="toc-text">UA伪装：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BD%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-number">5.2.</span> <span class="toc-text">3.2 动态加载的数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E7%BD%91%E4%B8%AD%E7%9A%84%E7%94%B5%E5%BD%B1%E8%AF%A6%E6%83%85%E6%95%B0%E6%8D%AE"><span class="toc-number">5.2.1.</span> <span class="toc-text">#爬取豆瓣网中的电影详情数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%82%AF%E5%BE%B7%E5%9F%BA%E9%A4%90%E5%8E%85%E6%9F%A5%E8%AF%A2"><span class="toc-number">5.2.2.</span> <span class="toc-text">#肯德基餐厅查询</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E8%8D%AF%E7%9B%91%E6%80%BB%E5%B1%80%E4%B8%AD%E7%9A%84%E4%BC%81%E4%B8%9A%E8%AF%A6%E6%83%85%E6%95%B0%E6%8D%AE"><span class="toc-number">5.2.3.</span> <span class="toc-text">#爬取药监总局中的企业详情数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E7%88%AC%E5%8F%96%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE"><span class="toc-number">5.3.</span> <span class="toc-text">3.3 爬取图片数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AB%99%E9%95%BF%E7%B4%A0%E6%9D%90%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-number">5.3.1.</span> <span class="toc-text">#站长素材图片数据爬取</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/30/%E7%88%AC%E8%99%AB-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%90day1%E3%80%91/" title="爬虫+数据分析【day1】"><img src="/img/loading.gif" data-original="/img/pa.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="爬虫+数据分析【day1】"/></a><div class="content"><a class="title" href="/2023/10/30/%E7%88%AC%E8%99%AB-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%90day1%E3%80%91/" title="爬虫+数据分析【day1】">爬虫+数据分析【day1】</a><time datetime="2023-10-30T14:07:29.000Z" title="发表于 2023-10-30 22:07:29">2023-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/18/%E7%94%A8vscode%E5%88%B7leetcode/" title="用vscode刷leetcode"><img src="/img/loading.gif" data-original="/img/vscode.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="用vscode刷leetcode"/></a><div class="content"><a class="title" href="/2023/10/18/%E7%94%A8vscode%E5%88%B7leetcode/" title="用vscode刷leetcode">用vscode刷leetcode</a><time datetime="2023-10-18T09:54:43.000Z" title="发表于 2023-10-18 17:54:43">2023-10-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/16/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" title="搭建个人博客"><img src="/img/loading.gif" data-original="/img/hexo.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="搭建个人博客"/></a><div class="content"><a class="title" href="/2023/10/16/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" title="搭建个人博客">搭建个人博客</a><time datetime="2023-10-16T13:38:48.000Z" title="发表于 2023-10-16 21:38:48">2023-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/22/hello-world/" title="Hello World"><img src="/img/loading.gif" data-original="/img/hello-1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2023/08/22/hello-world/" title="Hello World">Hello World</a><time datetime="2023-08-22T08:38:27.130Z" title="发表于 2023-08-22 16:38:27">2023-08-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/pa.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Capybara</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>