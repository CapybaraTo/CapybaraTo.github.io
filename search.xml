<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>爬虫+数据分析【day1】</title>
      <link href="/2023/10/30/%E7%88%AC%E8%99%AB-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%90day1%E3%80%91/"/>
      <url>/2023/10/30/%E7%88%AC%E8%99%AB-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%90day1%E3%80%91/</url>
      
        <content type="html"><![CDATA[<h1 id="学习方法"><a href="#学习方法" class="headerlink" title="学习方法"></a>学习方法</h1><p>不要边听边记，不要花大把的时间记笔记。照抄笔记没有意义。</p><p>记忆的不深刻：用自己的话术记笔记，自己的话术要白话一点。只有自己有了深刻的理解，才能把它用白话讲出来，不理解才会背专业的话术。</p><p>课上一点笔记都不要做，课下看一遍老师的笔记，并按自己的理解与话术白话一点地做一个总结即可。不要过于担心自己的思路偏差，只要自己总结的概念在使用过程中没有任何问题就是对的思路。</p><h1 id="day1"><a href="#day1" class="headerlink" title="day1"></a>day1</h1><h1 id="1-开发环境介绍"><a href="#1-开发环境介绍" class="headerlink" title="1 开发环境介绍"></a>1 开发环境介绍</h1><ul><li><p><strong>anaconda</strong>：基于数据分析和机器学习的<strong>集成环境</strong></p></li><li><p><strong>jupyter</strong>：anaconda 提供的一个基于浏览器的可视化开发的工具</p></li></ul><h1 id="2-jupyter的基本使用"><a href="#2-jupyter的基本使用" class="headerlink" title="2 jupyter的基本使用"></a>2 jupyter的基本使用</h1><ul><li><p>在终端录入jupyter notebook的指令启动jupyter可视化的开发工具</p></li><li><p>jupyter notebook的指令录入对应的默认的目录结构就是终端对应的目录结构</p></li><li><p>new-&gt;text file：新建一个任意后缀的文本文件，可以写代码但不能执行，在终端里利用python工作可以执行。</p></li><li><p>new-&gt;python 3 ：新建一个基于jupyter的源文件（xxx.ipynb）</p></li><li><p>ceil：jupyter源文件中的一个编辑行。</p></li><li><p>ceil是可以分为两种不同的模式：</p><ul><li>code模式：用来编写和执行代码</li><li>mardown模式：编写笔记。里面写代码不能执行</li></ul></li><li><p>快捷键的使用</p><ul><li><p>插入ceil：按a、b</p></li><li><p>删除ceil：x</p></li><li><p>撤销上一次：z</p></li><li><p>执行ceil：shift+enter  不论是code模式还是markdown模式都需要执行才能看到最后的结果。</p></li><li><p>切换ceil的模式：</p><ul><li>y：将markdown模式的ceil切换成code模式</li><li>m：将code切换为markdown</li></ul></li><li><p>打开帮助文档</p><ul><li>shift+tab：快速知道方法的含义、参数、例子等</li></ul></li></ul></li></ul><h1 id="3-爬虫"><a href="#3-爬虫" class="headerlink" title="3 爬虫"></a>3 爬虫</h1><p>概念：就是通过编写程序，让其模拟浏览器上网，然后去互联网上抓取数据的过程</p><ul><li>模拟：浏览器就是一款天然的爬虫工具!</li><li>抓取:：抓取一整张数据，抓取一整张数据中的局部数据</li></ul><p>爬虫的分类:</p><ul><li>通用爬虫（数据的爬取）：抓取一整张页面源码数据</li><li>聚焦爬虫（数据解析）：抓取局部的指定的数据。基于通用爬虫</li><li>增量式爬虫（数据的更新）：监测网站数据更新的情况!抓取网站最新更新出来的数据。</li><li>分布式：</li></ul><p>反爬机制</p><ul><li>一些网站的后台会设定相关的机制阻止爬虫程序进行数据的爬取。</li></ul><p>反反爬策略</p><ul><li>爬虫需要制定相关的策略破解反爬机制，从而可以爬取到网站的数据。</li></ul><p><strong>第一个反爬机制</strong></p><ul><li>robots协议: 存在于网站服务器的一个文本协议。指明了该网站中哪些数据可以爬取哪些不可以爬取。（防君子不防小人）</li></ul><img src="/images/image-20231016165552576.png" alt="image-20231016165552576" style="zoom:50%;" /><p>爬虫风险：</p><ul><li>爬虫干扰了被访问网站的正常运营;</li><li>爬虫抓取了受到法律保护的特定类型的数据或信息</li></ul><h2 id="3-1-requests的基本操作"><a href="#3-1-requests的基本操作" class="headerlink" title="3.1 requests的基本操作"></a>3.1 requests的基本操作</h2><p>urllib模块：基于模拟浏览器上网的模块，网络请求模块。（基本没人用了）</p><p>requests模块：基于网络请求的模块。作用：模拟浏览器上网。</p><p>requests模块的编码流程：</p><ol><li>指定url</li><li>发起请求</li><li>获取响应数据（爬取的数据）</li><li>对数据持久化存储</li></ol><h3 id="爬取搜狗首页的页面源码数据"><a href="#爬取搜狗首页的页面源码数据" class="headerlink" title="#爬取搜狗首页的页面源码数据"></a><strong>#爬取搜狗首页的页面源码数据</strong></h3><pre><code>#1.爬取搜狗首页的页面源码数据url = &#39;https://www.sogou.com/&#39;response = requests.get(url=url)page_text = response.text #text返回的是字符串形式的响应数据with open(&#39;./sogou.html&#39;,&#39;w&#39;,encoding=&#39;utf-8&#39;) as fp:    fp.write(page_text)   //写到文件里#分别对应如上四步</code></pre><p><code>with open() as fp:</code>是 Python 中打开文件的一种常用方式。它的作用是打开一个文件，并将其赋值给一个变量 fp。</p><p> with关键字 可以在语句结束后，关闭文件流。不用with关键字，文件会被python垃圾回收关闭。</p><p>open(‘file’, mode)第一个参数是包含文件名的字符串。第二个参数是另一个字符串，其中包含一些描述文件使用方式的字符，如 ‘r’，表示文件只能读取，‘w’ 表示只能写入。<code>encoding=&#39;utf-8&#39;</code>以utf-8的格式存入</p><h3 id="简易的网页采集器"><a href="#简易的网页采集器" class="headerlink" title="#简易的网页采集器"></a><strong>#简易的网页采集器</strong></h3><pre><code>#2.简易的网页采集器#涉及到的知识点：参数动态化，UA伪装，乱码的处理word = input(&#39;enter a key word:&#39;)  //输入我想搜素的url = &#39;https://www.sogou.com/web&#39;#参数动态化：将请求参数封装成字典作用到get方法的params参数中params = &#123;    &#39;query&#39;:word&#125;response = requests.get(url=url,params=params)page_text = response.textfileName = word+&#39;.html&#39;   //起一个名字，存爬取数据的文件with open(fileName,&#39;w&#39;,encoding=&#39;utf-8&#39;) as fp:    fp.write(page_text)print(word,&#39;下载成功！！！&#39;)</code></pre><p><code>url = &#39;https://www.sogou.com/web?query=jay&#39;</code> 只能请求jay的，想我输入什么就请求什么–&gt;参数动态化</p><ul><li>上述代码出现的问题：<ul><li><strong>乱码问题</strong></li><li><strong>爬取数据丢失</strong></li></ul></li></ul><pre><code>#乱码处理word = input(&#39;enter a key word:&#39;)url = &#39;https://www.sogou.com/web&#39;#参数动态化：将请求参数封装成字典作用到get方法的params参数中params = &#123;    &#39;query&#39;:word&#125;response = requests.get(url=url,params=params)#可以修改响应数据的编码response.encoding = &#39;utf-8&#39;#手动修改了响应对象的编码格式page_text = response.textfileName = word+&#39;.html&#39;with open(fileName,&#39;w&#39;,encoding=&#39;utf-8&#39;) as fp:    fp.write(page_text)print(word,&#39;下载成功！！！&#39;)</code></pre><p>显示 检测到异常的访问请求。</p><ul><li><p>什么叫做异常的访问请求？</p><ul><li>在爬虫中正常的访问请求指的是通过真实的浏览器发起的访问请求。</li><li>异常的访问请求：通过非浏览器发起的请求。（爬虫程序模拟的请求发送）</li></ul></li><li><p>正常的访问请求和异常的访问请求的判别方式是什么？</p><ul><li>是通过请求头中的User-Agent判别。</li><li>User-Agent：请求载体的身份标识</li><li>目前请求的载体可以是：浏览器，爬虫</li></ul></li><li><p>反爬机制：</p><ul><li>UA检测：网站后台会检测请求载体的身份标识（UA）是不是浏览器。<ul><li>是：正常的访问请求</li><li>不是：异常的访问请求</li></ul></li></ul></li><li><p>反反爬策略：</p><ul><li><h3 id="UA伪装："><a href="#UA伪装：" class="headerlink" title="UA伪装："></a><strong>UA伪装</strong>：</h3><ul><li>大部分网站都需要进行下UA伪装</li><li>将爬虫对应的请求载体身份标识伪装&#x2F;篡改成浏览器的身份标识</li><li>请求的header里的User-Agent: Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;80.0.3987.122 Safari&#x2F;537.36    不同的浏览器是不同的这个字符串</li></ul></li></ul></li></ul><pre><code>#UA伪装word = input(&#39;enter a key word:&#39;)url = &#39;https://www.sogou.com/web&#39;#参数动态化：将请求参数封装成字典作用到get方法的params参数中params = &#123;    &#39;query&#39;:word&#125;#UA伪装headers = &#123;    &quot;User-Agent&quot;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36&#39;&#125;#将伪装的UA作用到了请求的请求头中response = requests.get(url=url,params=params,headers=headers)#可以修改响应数据的编码response.encoding = &#39;utf-8&#39;#手动修改了响应对象的编码格式page_text = response.textfileName = word+&#39;.html&#39;with open(fileName,&#39;w&#39;,encoding=&#39;utf-8&#39;) as fp:    fp.write(page_text)print(word,&#39;下载成功！！！&#39;)</code></pre><p><strong>&#x3D;&#x3D;总结&#x3D;&#x3D;：</strong></p><ul><li>get方法的参数：<ul><li>url</li><li>params</li><li>headers</li></ul></li><li>get方法的返回值：<ul><li>response</li></ul></li><li>response的属性：<ul><li>text：字符串形式的响应数据</li></ul></li></ul><h2 id="3-2-动态加载的数据"><a href="#3-2-动态加载的数据" class="headerlink" title="3.2 动态加载的数据"></a>3.2 动态加载的数据</h2><h3 id="爬取豆瓣网中的电影详情数据"><a href="#爬取豆瓣网中的电影详情数据" class="headerlink" title="#爬取豆瓣网中的电影详情数据"></a><strong>#爬取豆瓣网中的电影详情数据</strong></h3><ul><li>url：<a href="https://movie.douban.com/typerank?type_name=%E7%A7%91%E5%B9%BB&type=17&interval_id=100:90&action=">https://movie.douban.com/typerank?type_name&#x3D;%E7%A7%91%E5%B9%BB&amp;type&#x3D;17&amp;interval_id&#x3D;100:90&amp;action&#x3D;</a></li><li>涉及到的重点：动态加载数据</li></ul><p>滚轮向下不断地加载有新的数据。网页地址没变点但页面刷新，做的是一个局部刷新，发动的是ajax的请求。</p><p><strong>分析网站</strong>：</p><ul><li>当滚轮滑动到底部的时候，页面会发起ajax请求，且请求到一组电影详情数据。</li><li>当滚轮不滑动的时候，页面显示的电影数据通过对浏览器地址栏的url发起请求是请求不到的。（当要爬数据时，先从开发者工具里看看当前url请求下的response有没有要爬取的信息）</li></ul><p><strong>动态加载的数据</strong></p><ul><li>可见非即可得</li><li><strong>当我们对一个陌生的网站进行指定数据爬取之前，我们在写代码之前必须要做的一个事情就是校验你想要爬取的数据是否为动态加载的数据</strong>。</li><li>概念：通过非浏览器地址栏url请求到的数据（另外的一个新的请求请求到的数据），即地址栏的url实际上请求不到的<ul><li>是动态加载的数据<ul><li>基于抓包工具进行全局搜索，锁定动态加载数据对应的数据包即可。从数据包中提取请求的url和请求方式和请求参数。</li><li>怎么做：在任意一个network数据包上ctrl+F 搜索动态加载的数据定位</li></ul></li><li>不是动态加载的数据<ul><li>直接对地址栏的url发起请求就可以获取指定数据</li></ul></li></ul></li></ul><p>response.text返回的是字符串数据，序列化，用response.json()，就不会返回一个字符串对象，而是一个列表</p><p><img src="/images/image-20231016193441251.png" alt="image-20231016193441251"></p><p>该数据包的url是<code>&#39;https://movie.douban.com/j/chart/top_list&#39;</code>，该数据包是<strong>get请求</strong>。</p><pre><code>url = &#39;https://movie.douban.com/j/chart/top_list&#39;#参数动态化params = &#123;    &quot;type&quot;: &quot;17&quot;,       &quot;interval_id&quot;: &quot;100:90&quot;,    &quot;action&quot;: &quot;&quot;,    &quot;start&quot;: &quot;20&quot;,    &quot;limit&quot;: &quot;10&quot;,&#125;response = requests.get(url=url,headers=headers,params=params)page_text = response.json() #json返回的是序列号好的对象#将电影名称和评分进行解析for dic in page_text:    name = dic[&#39;title&#39;]    score = dic[&#39;score&#39;]    print(name+&#39;:&#39;+score)</code></pre><img src="/images/image-20231016193908951.png" alt="image-20231016193908951" style="zoom: 50%;" /><p>**&#x3D;&#x3D;总结&#x3D;&#x3D;**：</p><ul><li>问题：如何检测页面中的数据是否为动态加载的数据？<ul><li>基于抓包工具进行局部搜索<ul><li>搜索到：不是动态加载数据</li><li>搜索不到：是动态加载数据</li></ul></li></ul></li></ul><h3 id="肯德基餐厅查询"><a href="#肯德基餐厅查询" class="headerlink" title="#肯德基餐厅查询"></a>#<strong>肯德基餐厅查询</strong></h3><ul><li><p>肯德基餐厅查询：<a href="http://www.kfc.com.cn/kfccda/storelist/index.aspx">http://www.kfc.com.cn/kfccda/storelist/index.aspx</a></p></li><li><p>分析：</p><ul><li><p>通过测试发现，数据为动态加载数据</p></li><li><p>通过抓包工具的全局搜索捕获动态加载数据</p><p>全局搜索后，url是’<a href="http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword%27%EF%BC%8C%E8%AF%B7%E6%B1%82%E6%98%AFpost%E8%AF%B7%E6%B1%82%EF%BC%8C%E6%90%BA%E5%B8%A6%E4%BA%86%E5%8F%82%E6%95%B0%E3%80%82data%E5%B0%B1%E6%98%AF%E8%AF%B7%E6%B1%82%E7%9A%84%E5%8F%82%E6%95%B0%EF%BC%8C%E5%B0%81%E8%A3%85%E4%B8%BA%E9%94%AE%E5%80%BC%E5%AF%B9%E7%9A%84%E5%BD%A2%E5%BC%8F%E3%80%82">http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&#39;，请求是post请求，携带了参数。data就是请求的参数，封装为键值对的形式。</a></p><p>基本所有的requests都要带上headers</p></li></ul></li></ul><pre><code>url = &#39;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&#39;data = &#123;    &quot;cname&quot;: &quot;&quot;,    &quot;pid&quot;: &quot;&quot;,    &quot;keyword&quot;: &quot;北京&quot;,    &quot;pageIndex&quot;: &quot;1&quot;,    &quot;pageSize&quot;: &quot;10&quot;,&#125;#参数：data是用来实现参数动态化，等同于get方法中的params参数的作用response = requests.post(url=url,headers=headers,data=data)page_text = response.json()for dic in page_text[&#39;Table1&#39;]:    pos = dic[&#39;addressDetail&#39;]    print(pos)</code></pre><p>目前只有当前页面的10条，想获取7页所有的数据</p><pre><code>#想要获取所有页码对应的位置信息url = &#39;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&#39;for pageNum in range(1,8):    data = &#123;        &quot;cname&quot;: &quot;&quot;,        &quot;pid&quot;: &quot;&quot;,        &quot;keyword&quot;: &quot;北京&quot;,        &quot;pageIndex&quot;: str(pageNum),        &quot;pageSize&quot;: &quot;10&quot;,    &#125;    #参数：data是用来实现参数动态化，等同于get方法中的params参数的作用    response = requests.post(url=url,headers=headers,data=data)    page_text = response.json()    for dic in page_text[&#39;Table1&#39;]:        pos = dic[&#39;addressDetail&#39;]        print(pos)</code></pre><h3 id="爬取药监总局中的企业详情数据"><a href="#爬取药监总局中的企业详情数据" class="headerlink" title="#爬取药监总局中的企业详情数据"></a>#<strong>爬取药监总局中的企业详情数据</strong></h3><ul><li><p>需求：爬取药监总局中的企业详情数据，每一家企业详情页对应的详情数据（爬取前5页企业）</p></li><li><p>url：<a href="http://125.35.6.84:81/xk/">http://125.35.6.84:81/xk/</a></p></li><li><p>分析：</p><ul><li><strong>先看企业详情页数据</strong>是否为动态加载数据？<ul><li>基于抓包工具进行局部搜索。发现为动态加载数据</li></ul></li><li>捕获动态加载的数据<ul><li>基于抓包工具进行全局搜索。</li><li>定位到的数据包提取的<ul><li>url：<ul><li><a href="http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById">http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById</a></li><li><a href="http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById">http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById</a></li></ul></li><li>请求参数：<ul><li>id: 536878abac734332ae06dcb1a3fbd14a</li><li>id: 950d66fbf8714fbc9e799010e483d2d5</li></ul></li></ul></li><li>结论：每一家企业详情数据对应的请求url和请求方式都是一样的，只有请求参数id的值不一样。</li><li><strong>如果我们可以将每一家企业的id值捕获，则就可以将每一家企业详情数据进行爬取。</strong></li></ul></li><li>捕获企业的id<ul><li>企业的id表示的就是唯一的一家企业。我们就猜测企业id可能会和企业名称捆绑在一起。</li><li>在首页中会有不同的企业名称，则我们就基于抓包工具对首页的数据包进行全局搜索（企业名称）<ul><li>url：<a href="http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList">http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList</a></li><li>方式：post</li><li>请求参数：<ul><li>on&#x3D;true&amp;page&#x3D;1&amp;pageSize&#x3D;15&amp;productName&#x3D;&amp;conditionType&#x3D;1&amp;applyname&#x3D;&amp;applysn&#x3D;</li></ul></li></ul></li></ul></li></ul></li></ul><pre><code>#获取每一家企业的id值，去首页分析查找对应企业的id值url = &#39;http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList&#39;data = &#123;    &#39;on&#39;: &#39;true&#39;,    &#39;page&#39;: &#39;1&#39;,    &#39;pageSize&#39;: &#39;15&#39;,    &#39;productName&#39;: &#39;&#39;,    &#39;conditionType&#39;: &#39;1&#39;,    &#39;applyname&#39;: &#39;&#39;,    &#39;applysn&#39;: &#39;&#39;,&#125;response = requests.post(url=url,headers=headers,data=data)all_company_list = response.json()[&#39;list&#39;]for dic in all_company_list:    _id = dic[&#39;ID&#39;]#     print(_id)    #将id作为请求企业详情数据url的请求参数  在for循环内    detail_url = &#39;http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById&#39;    data = &#123;        &#39;id&#39;:_id    &#125;    response = requests.post(url=detail_url,headers=headers,data=data)    company_detail_dic = response.json()    person_name = company_detail_dic[&#39;businessPerson&#39;]    addr = company_detail_dic[&#39;epsProductAddress&#39;]    print(person_name,addr)</code></pre><pre><code>#捕获多页数据#获取每一家企业的id值，去首页分析查找对应企业的id值url = &#39;http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList&#39;for page in range(1,6):    data = &#123;        &#39;on&#39;: &#39;true&#39;,        &#39;page&#39;: str(page),        &#39;pageSize&#39;: &#39;15&#39;,        &#39;productName&#39;: &#39;&#39;,        &#39;conditionType&#39;: &#39;1&#39;,        &#39;applyname&#39;: &#39;&#39;,        &#39;applysn&#39;: &#39;&#39;,    &#125;    response = requests.post(url=url,headers=headers,data=data)    all_company_list = response.json()[&#39;list&#39;]    for dic in all_company_list:        _id = dic[&#39;ID&#39;]    #     print(_id)        #将id作为请求企业详情数据url的请求参数        detail_url = &#39;http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById&#39;        data = &#123;            &#39;id&#39;:_id        &#125;        response = requests.post(url=detail_url,headers=headers,data=data)        company_detail_dic = response.json()        person_name = company_detail_dic[&#39;businessPerson&#39;]        addr = company_detail_dic[&#39;epsProductAddress&#39;]        print(person_name,addr)</code></pre><p>data和params的区别，**&#x3D;&#x3D;data对应post方法，params对应get方法&#x3D;&#x3D;**，区别仅此而已，没有其他区别</p><h2 id="3-3-爬取图片数据"><a href="#3-3-爬取图片数据" class="headerlink" title="3.3 爬取图片数据"></a>3.3 爬取图片数据</h2><h3 id="站长素材图片数据爬取"><a href="#站长素材图片数据爬取" class="headerlink" title="#站长素材图片数据爬取"></a>#站长素材图片数据爬取</h3><p>两种方式：</p><ul><li>方式1：requests</li><li>方式2：urllib</li></ul><pre><code>#requestsurl = &#39;http://pics.sc.chinaz.com/files/pic/pic9/201908/zzpic19447.jpg&#39;response = requests.get(url=url,headers=headers)img_data = response.content #content返回的是bytes类型的响应数据with open(&#39;./123.png&#39;,&#39;wb&#39;) as fp:    fp.write(img_data)</code></pre><pre><code>#urllibfrom urllib import requesturl = &#39;http://pics.sc.chinaz.com/files/pic/pic9/201908/zzpic19447.jpg&#39;request.urlretrieve(url=url,filename=&#39;./456.png&#39;)</code></pre><p>注意#urllib 就是<code>request</code>没有s</p><p><code>request.urlretrieve</code></p><ul><li>问题：两种图片爬取的方式的主要区别有哪些？<ul><li>requests的方式可以实现UA伪装，而urlib无法实现UA伪装</li></ul></li></ul><p>如何批量获取图片url地址</p><p>用正则进行数据解析</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>用vscode刷leetcode</title>
      <link href="/2023/10/18/%E7%94%A8vscode%E5%88%B7leetcode/"/>
      <url>/2023/10/18/%E7%94%A8vscode%E5%88%B7leetcode/</url>
      
        <content type="html"><![CDATA[<p>想起本科时准备蓝桥杯时，还是本地用devc++调试然后再改代码在leetcode上运行提交，着实小白了。正值快要找实习了，刷题还是需要尽早提上日程，好在现在知道了有更好的工具，我们就尽量用工具，不必被一些有的没的给自己添加麻烦。</p><h1 id="1-下载vscode"><a href="#1-下载vscode" class="headerlink" title="1 下载vscode"></a>1 下载vscode</h1><p>我自己下的已有的就不多赘述了。主要看用什么语言刷题，需要下载对应的扩展，本人还是用的c++，如图。</p><p><img src="/images/image20231018171822020.png" alt="image-20231018171822020"></p><h1 id="2-配置环境"><a href="#2-配置环境" class="headerlink" title="2 配置环境"></a>2 配置环境</h1><h2 id="2-1-安装MinGW编译器"><a href="#2-1-安装MinGW编译器" class="headerlink" title="2.1 安装MinGW编译器"></a>2.1 安装<a href="https://so.csdn.net/so/search?q=MinGW&spm=1001.2101.3001.7020">MinGW</a>编译器</h2><p><a href="https://sourceforge.net/projects/mingw-w64/files/mingw-w64/mingw-w64-release/">https://sourceforge.net/projects/mingw-w64/files/mingw-w64/mingw-w64-release/</a></p><p><img src="/images/image20231018172130793.png" alt="image-20231018172130793"></p><p>绿框默认下载的是压缩包，如果想下载运行文件就拖到最后。但是不建议下载exe文件进行安装，不知道为什么我新换的这个电脑屡次安装失败。但是压缩包解压后，并添加环境变量是很顺利的。</p><img src="/images/image20231018172207499.png" alt="image-20231018172207499" style="zoom: 50%;" /><p>下载后打开下载的压缩包所在文件夹，解压到一个包含中文文字的路径目录下，压缩包根据自己心情决定是否删除。如：当然D盘也没问题，只是我的D盘快满了，C盘还很富裕（谁能想到）。</p><img src="/images/image20231018172522710.png" alt="image-20231018172522710" style="zoom: 50%;" /><h2 id="2-2-配置环境变量"><a href="#2-2-配置环境变量" class="headerlink" title="2.2 配置环境变量"></a>2.2 配置环境变量</h2><p>复制上面这个文件夹下bin的路径，如我的：C:\Program Files\mingw64\bin</p><p>在<strong>系统环境变量</strong>中的Path中新建，将复制的路径贴入，确定，确定，确定。</p><p>验证下是否可用，在cmd中键入gcc -v看反没反馈信息。</p><h1 id="3-vscode安装leetcode插件"><a href="#3-vscode安装leetcode插件" class="headerlink" title="3 vscode安装leetcode插件"></a>3 vscode安装leetcode插件</h1><p><img src="/images/image20231018172919486.png" alt="image-20231018172919486"></p><p>成功后左边栏会出现这个图标<img src="/images/image20231018173147429.png" alt="image-20231018173147429" style="zoom:33%;" /></p><h1 id="4-安装nodejs"><a href="#4-安装nodejs" class="headerlink" title="4 安装nodejs"></a>4 安装nodejs</h1><p>我的因为之前编写前端工作，nodejs已有，也不过多赘述。下载安装也比较方便。</p><p><a href="https://nodejs.org/en">https://nodejs.org/en</a></p><p><img src="/images/image20231018173037419.png" alt="image-20231018173037419"></p><p>下载完成后打开，除了安装路径一路next即可。Node.js在安装完成之后会自动配置环境变量，所以我们无需操心，但需要记住安装位置，后边需要使用到。</p><h1 id="5-登录leetcode账号"><a href="#5-登录leetcode账号" class="headerlink" title="5 登录leetcode账号"></a>5 登录leetcode账号</h1><h2 id="5-1-修改站点为中国leetcode"><a href="#5-1-修改站点为中国leetcode" class="headerlink" title="5.1 修改站点为中国leetcode"></a>5.1 修改站点为中国leetcode</h2><img src="/images/image20231018173316871.png" alt="image-20231018173316871" style="zoom:50%;" /><h2 id="5-2-登录账户"><a href="#5-2-登录账户" class="headerlink" title="5.2 登录账户"></a>5.2 登录账户</h2><p>点击sign in to leetcode按钮，输入自己的账号密码即可登录。</p><h2 id="5-3-配置文件路径和nodejs路径"><a href="#5-3-配置文件路径和nodejs路径" class="headerlink" title="5.3 配置文件路径和nodejs路径"></a>5.3 配置文件路径和nodejs路径</h2><p>编程文件都会保存到本地，默认路径为“$HOME.leetcode”，可以自行设置路径。<br>点击扩展按钮，选中LeetCode插件，鼠标右键选择<strong>扩展设置</strong>。<br>找到Node Path（node所在路径）和Workspace Folder（自己想要保存的leetcode编程记录文件地址），选择相应路径。然后就可以开始快乐编程了。</p><h1 id="6-编写"><a href="#6-编写" class="headerlink" title="6 编写"></a>6 编写</h1><p>左边栏会把各种题目进行了分类，点进一个题目，可以看到题目描述，点击右下角的code now就可以编代码了。你发现他的类给你封装好了，不用再从#include<iostream>开始了（狗头）。</p><p><img src="/images/image20231018173704371.png" alt="image-20231018173704371"></p><p>点击test可以测试，测试用例有默认录好的。点击submit就可以提交，提交情况会同步到账号，也就是网页登录都会显示已提交。</p><p><img src="/images/image20231018173847292.png" alt="image-20231018173847292"></p><h1 id="7-调试"><a href="#7-调试" class="headerlink" title="7 调试"></a>7 调试</h1><p>上面的做完其实只有submit和test按钮，那我们用vscode刷题的便捷优势还没有凸显出来。想要调试，还得进行一些配置：</p><p>下载debug leetcode，就完成啦。</p><p><img src="/images/image20231018174746878.png" alt="image-20231018174746878"></p><p><img src="/images/image20231018180114403.png" alt="image-20231018180114403"></p><h1 id="遇到的一些问题"><a href="#遇到的一些问题" class="headerlink" title="遇到的一些问题"></a>遇到的一些问题</h1><p>其实编程语言环境配好基本没什么问题，留意上面都是c++，如果用java，插件还是有一点区别的。</p><p>还遇到一个问题就是，ListNode、cout等关键字爆红，显示为 未定义标识符，这是c++扩展设置的问题。如下方式解决：<br>File-》Preference-》Setting-》Extensions-》C&#x2F;C++<br>“C_Cpp.intelliSenseEngine”: “Default”<br>改为”C_Cpp.intelliSenseEngine”: “Tag Parser”</p><p><img src="/images/image20231017100516252.png" alt="image-20231017100516252"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>搭建个人博客</title>
      <link href="/2023/10/16/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2023/10/16/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<p>抽出时间来记录下个人博客搭建。通过github page平台上托管博客，hexo作为一个快速简洁的博客框架，用它来搭建博客还挺非常容易得。</p><h1 id="Hexo简介"><a href="#Hexo简介" class="headerlink" title="Hexo简介"></a>Hexo简介</h1><p>Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。可以进入<a href="https://link.zhihu.com/?target=https://hexo.io/zh-cn/">hexo官网</a>进行详细查看，因为Hexo的创建者是台湾人，对中文的支持很友好，可以选择中文进行查看。</p><p>本文分三个部分，</p><ul><li>第一部分：hexo搭建及部署</li><li>第二部分：更换主题、设置主题</li><li>第三部分：推送文章</li></ul><h1 id="第一部分：搭建"><a href="#第一部分：搭建" class="headerlink" title="第一部分：搭建"></a>第一部分：搭建</h1><p>hexo的初级搭建还有部署到github page上，以及个人域名的绑定。</p><p><strong>Hexo搭建步骤</strong></p><ol><li>安装Git</li><li>安装Node.js</li><li>安装Hexo</li><li>GitHub创建个人仓库</li><li>生成SSH添加到GitHub</li><li>将hexo部署到GitHub</li><li>设置个人域名</li><li>发布文章</li></ol><h2 id="1-1-安装Hexo"><a href="#1-1-安装Hexo" class="headerlink" title="1.1 安装Hexo"></a>1.1 安装Hexo</h2><p>前两步骤 安装Git、安装Node.js 暂时跳过，电脑已有相应的环境。</p><p>留意一下版本问题，如下是hexo及对应的最低兼容node.js的版本：</p><table><thead><tr><th>Hexo 版本</th><th>最低兼容 Node.js 版本</th></tr></thead><tbody><tr><td>6.0+</td><td>12.13.0</td></tr><tr><td>5.0+</td><td>10.13.0</td></tr><tr><td>4.1 - 4.2</td><td>8.10</td></tr><tr><td>4.0</td><td>8.6</td></tr><tr><td>3.3 - 3.9</td><td>6.9</td></tr><tr><td>3.2 - 3.3</td><td>0.12</td></tr><tr><td>3.0 - 3.1</td><td>0.10 or iojs</td></tr><tr><td>0.0.1 - 2.8</td><td>0.10</td></tr></tbody></table><p>首先创建一个文件夹blog，在这个文件夹下右键git bash打开。</p><p>输入命令安装hexo</p><pre><code>npm install -g hexo-cli</code></pre><p>装完可以用<code>hexo -v</code>查看一下版本。</p><p>初始化hexo</p><pre><code>hexo init myblog  </code></pre><p>myblog可以自己取什么名字都行</p><pre><code>cd myblog //进入这个myblog文件夹npm install  //安装依赖</code></pre><p>新建完成后，指定文件夹目录下有：</p><ul><li>node_modules: 依赖包</li><li>public：存放生成的页面</li><li>scaffolds：生成文章的一些模板</li><li>source：用来存放你的文章</li><li>themes：主题</li><li>** _config.yml: 博客的配置文件**</li></ul><pre><code>hexo ghexo server</code></pre><p>打开hexo的服务，在浏览器输入localhost:4000就可以看到你生成的博客了。</p><p><img src="/images/image20231016195803091.png" alt="image-20231016195803091"></p><h2 id="1-2-GitHub创建个人仓库"><a href="#1-2-GitHub创建个人仓库" class="headerlink" title="1.2 GitHub创建个人仓库"></a>1.2 GitHub创建个人仓库</h2><p>在github新建一个仓库。</p><ul><li>新建一个名为: <a href="https://link.zhihu.com/?target=http://username.github.io">http://username.github.io</a> 的仓库(username 为你的 Github 用户名)</li><li>必须是用户名，其它名称无效，将来博客的网站访问地址就是 <a href="https://link.zhihu.com/?target=https://test.github.io/">https://test.github.io</a> 了。由此可见，每一个 github 账户最多只能创建一个这样可以直接使用域名访问的仓库。</li></ul><img src="/images/image20231016195945050.png" alt="image20231016195945050" style="zoom: 67%;" /><img src="/images/image20231016200310797.png" alt="image20231016200310797" style="zoom:67%;" /><img src="/images/image20231016200219125.png" alt="image20231016200219125" style="zoom:67%;" /><h2 id="1-3-生成SSH添加到GitHub"><a href="#1-3-生成SSH添加到GitHub" class="headerlink" title="1.3 生成SSH添加到GitHub"></a>1.3 生成SSH添加到GitHub</h2><p>将SSH密钥添加到GitHub是为了通过SSH协议进行安全的身份验证，而不是使用用户名和密码。这提供了更高的安全性，并允许您无需输入密码即可访问和推送GitHub存储库。这对于开发者在与GitHub进行交互时更加方便和安全。</p><p>首先打开电脑文件夹，找到 C:\Users\您的用户名\ .ssh文件夹并删除(如果没有，则直接进入第二步)。</p><p>在 C:\Users\您的用户名 文件夹下右键打开 Git Bash输入命令: <code>ssh-keygen -t rsa -C &quot;你的github登录邮箱&quot;</code> 生成.ssh秘钥，输入后连敲三次回车，出现下图情况代表成功。</p><p><img src="/images/image20231016200749843.png" alt="image-20231016200749843"></p><p>生成了一个新的 C:\Users\您的用户名\ .ssh文件夹，打开这个文件夹，找到 .ssh\id_rsa.pub 文件，记事本打开并复制里面的内容</p><p>打开您的 github 主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key，把复制的内容粘贴进去，title 随便填，保存即可，我们的公钥就添加成功了，设置好如下图:</p><p><img src="/images/image20231016200853748.png" alt="image-20231016200853748"></p><p>检查一下是否设置成功</p><pre><code>ssh -T git@github.com</code></pre><p>还需要如下配置</p><pre><code>git config --global user.name &quot;yourname&quot;git config --global user.email &quot;youremail&quot;</code></pre><h2 id="1-4-将hexo部署到GitHub"><a href="#1-4-将hexo部署到GitHub" class="headerlink" title="1.4 将hexo部署到GitHub"></a>1.4 将hexo部署到GitHub</h2><p>在blog目录下安装 hexo-deployer-git 插件</p><pre><code>npm install hexo-deployer-git --save</code></pre><p>编辑blog 目录下的 _config.yml 文件，在文件末尾添加如下内容。翻到最后，修改为 YourgithubName就是你的GitHub账户</p><pre><code>deploy:  type: git  repo: https://github.com/YourgithubName/YourgithubName.github.io.git  branch: master</code></pre><p>然后通过：</p><pre><code>hexo cleanhexo generatehexo deploy</code></pre><p>其中 <code>hexo clean</code>清除了之前生成的东西，也可以不加。 <code>hexo generate</code> 顾名思义，生成静态文章，可以用 <code>hexo g</code>缩写 <code>hexo deploy</code> 将本地 blog 推送到 github 远程仓库，也可能需要输入 username &amp; pwd</p><p>推送成功后，即可通过 <a href="https://username.github.io/">https://username.github.io/</a> 访问个人博客了!</p><h1 id="第二部分：主题"><a href="#第二部分：主题" class="headerlink" title="第二部分：主题"></a>第二部分：主题</h1><p>在blog目录下有一个 themes 文件夹，该文件夹下存放着 hexo 所使用的主题.</p><p>hexo 官方提供了很多主题供我们使用，地址: <a href="https://link.zhihu.com/?target=https://hexo.io/themes/">Themes | Hexo</a>, 选择喜欢的主题并点击即可跳转至 github</p><p>我使用了butterfly主题。</p><h2 id="2-1-下载主题"><a href="#2-1-下载主题" class="headerlink" title="2.1 下载主题"></a>2.1 下载主题</h2><p>在blog 目录下右键 Git Bash Here</p><p>执行命令: <code>git clone 主题http链接 themes/主题名称</code> 将主题下载至 themes 文件夹下</p><pre><code>git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly</code></pre><h2 id="2-2-使用主题"><a href="#2-2-使用主题" class="headerlink" title="2.2 使用主题"></a>2.2 使用主题</h2><p>安装<code>pug</code> 和 <code>stylus</code> 渲染器。</p><pre><code>npm install hexo-renderer-pug hexo-renderer-stylus --save</code></pre><p>修改项目根目录下的<code>_config.yml</code>文件（称为站点配置文件），开启主题。</p><pre><code>theme: butterfly</code></pre><p>升级建议：</p><p>为了減少升级主题带来的不便，我们可以把主题文件夹中的 <code>_config.yml</code> 重命名为 <code>_config.butterfly.yml</code>，复制到blog根目录下与<code>config.yml</code>同级。</p><p>Hexo会自动合并主题中的<code>_config.yml</code>和 <code>_config.butterfly.yml</code> ，如果存在同名配置，会使用<code>_config.butterfly.yml</code>的配置，其优先度较高。所以像和博客网址相关联的固定资料可以设置在<code>_config.yml</code>中，比如博客的标题、作者信息和邮箱等等资料，而和主题样式相关的配置放在 <code>_config.butterfly.yml</code> 中，那么在将来你想换一个主题是很方便的。</p><h2 id="2-3-设置博客"><a href="#2-3-设置博客" class="headerlink" title="2.3 设置博客"></a>2.3 设置博客</h2><p><strong>设置博客个人资料</strong></p><p>修改根目录下的站点配置文件<code>_config.yml</code>，可以修改网站各种<strong>资料，例如标题、副标题和语言</strong>等个人资料。</p><pre><code># Sitetitle: Capybarato #标题subtitle: &#39;&#39;#副标题description: &#39;不要气馁，我的小太阳&#39;#个性签名keywords:author: GuoJingjing   #作者language: zh-CN  #语言timezone: Asia/Beijing  #中国的时区</code></pre><p><strong>设置导航菜单</strong></p><p>修改主题配置文件 <code>_config.butterfly.yml</code></p><pre><code>menu:  主页: / || fas fa-home  博文 || fa fa-graduation-cap:    分类: /categories/ || fa fa-archive    标签: /tags/ || fa fa-tags    归档: /archives/ || fa fa-folder-open  生活 || fas fa-list:    分享: /shuoshuo/ || fa fa-comments-o    相册: /photos/ || fa fa-camera-retro  友链: /links/ || fa fa-link  留言板: /comment/ || fa fa-paper-plane  #留言板: /messageboard/ || fa fa-paper-plane  关于笔者: /about/ || fas fa-heart  </code></pre><p>效果图：</p><p><strong>代码块显示设置</strong></p><pre><code>highlight_theme: mac #  darker / pale night / light / ocean / mac / mac light / falsehighlight_copy: true # copy buttonhighlight_lang: true # show the code languagehighlight_shrink: false # true: shrink the code blocks / false: expand the code blocks | none: expand code blocks and hide the buttonhighlight_height_limit: false # unit: pxcode_word_wrap: true  #代码自动换行，关闭滚动条</code></pre><p>效果图：</p><p><strong>本地搜索功能</strong>：<br>安装搜索插件</p><pre><code>npm install hexo-generator-search --save</code></pre><p>主题配置文件 <code>_config.butterfly.yml</code>补充：</p><pre><code># Local searchlocal_search:  enable: true  labels:    input_placeholder: Search for Posts    hits_empty: &quot;We didn&#39;t find any results for the search: $&#123;query&#125;&quot; # 如果没有查到内容相关内容显示</code></pre><p><strong>创建文件夹</strong></p><p>分类：</p><pre><code>hexo new page categories</code></pre><p>会出现<code>source/categories/index.md</code>文件</p><p>标签</p><pre><code>hexo new page tags</code></pre><p>会出现<code>source/tags/index.md</code>文件</p><p><strong>修改副标题</strong></p><p>修改主题配置文件 <code>_config.butterfly.yml</code>:</p><pre><code># the subtitle on homepage (主頁subtitle)subtitle:  enable: true  # Typewriter Effect (开启打字效果)  effect: true  # loop (循環打字)  loop: true  # source調用第三方服務  # source: false 關閉調用  # source: 1  調用搏天api的隨機語錄（簡體）  # source: 2  調用一言網的一句話（簡體）  # source: 3  調用一句網（簡體）  # source: 4  調用今日詩詞（簡體）  # subtitle 會先顯示 source , 再顯示 sub 的內容  source: false  # 如果有英文逗号&#39; , &#39;,请使用转义字元 &amp;#44;  # 如果有英文双引号&#39; &quot; &#39;,请使用转义字元 &amp;quot;  # 开头不允許转义字元，如需要，请把整個句子用双引号包住  # 如果关闭打字效果，subtitle只会现示sub的第一行文字  sub:</code></pre><p>图片设置</p><p>图片可以用云链接或者本地路径：<code>/themes/butterfly/source/img</code>。修改主题配置文件<code>_config.butterfly.yml</code>：</p><pre><code># Favicon（网站图）favicon: /img/favicon.png</code></pre><p>图像</p><pre><code>avatar:  img: /img/avatar.jpg #图片路径  effect: false #头像会一直转圈  </code></pre><p>主页封面图片</p><pre><code># The banner image of home pageindex_img: /img/background.jpg</code></pre><p>文章详情页的顶部图片</p><h1 id="第三部分：博文"><a href="#第三部分：博文" class="headerlink" title="第三部分：博文"></a>第三部分：博文</h1><h2 id="2-1-使用-Typora-编写博客"><a href="#2-1-使用-Typora-编写博客" class="headerlink" title="2.1 使用 Typora 编写博客"></a>2.1 使用 Typora 编写博客</h2><p>在blog目录下，通过输入命令: <code>hexo new &quot;文章 title&quot;</code> 会在 &#x2F;source 文件夹下生成对应文章的 .md 文件，然后就可以通过 Typora 打开此文件编写文章并保存了</p><p>写完该篇文章后，依次输入以下命令:</p><p><code>hexo clean</code> 删除 public 文件夹，即删除旧的博客文章</p><p><code>hexo g</code> 生成 public 文件夹，即生成新的博客文章相关 html 文件</p><p><code>hexo d</code> 将博客推送到 github</p><h2 id="2-2-插入图片"><a href="#2-2-插入图片" class="headerlink" title="2.2 插入图片"></a>2.2 插入图片</h2><p>当 Hexo 项目中只用到少量图片时，可以将图片统一放在 source&#x2F;images 文件夹中，通过 markdown 语法访问它们。</p><p>首先移动图片到images文件夹下，并改为这个格式例如(&#x2F;images&#x2F;image20231016200853748.png)</p><h2 id="2-3-修改博文的一些配置"><a href="#2-3-修改博文的一些配置" class="headerlink" title="2.3 修改博文的一些配置"></a>2.3 修改博文的一些配置</h2><p>top_img: &#x2F;img&#x2F;hexo.png # 顶部背景图   存在themes\butterfly\source\img中<br>cover: &#x2F;img&#x2F;hexo.png   # 文章封面</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/08/22/hello-world/"/>
      <url>/2023/08/22/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
